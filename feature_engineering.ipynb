{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386f6094-226f-4af2-bc55-754c68ada2c8",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Feature Engineering Assignment</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f72a9-3416-4640-ac0e-15caae00be6a",
   "metadata": {},
   "source": [
    "# Assignment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77882cd1-791f-479d-a4e0-1a0460c9f5ef",
   "metadata": {},
   "source": [
    "### Q1. What is a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e55dcd-94f9-437d-805d-852f8a0e74b9",
   "metadata": {},
   "source": [
    "### **Answer :**  In machine learning, a parameter refers to a variable within a model that is learned from the data during the training process.     \n",
    "- Parameters determine how the model makes predictions based on the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a805a-3a4d-4c45-85a4-0e3c76a99cba",
   "metadata": {},
   "source": [
    "Q2) What is correlation?\n",
    "    What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244dee9-25ac-468d-a388-d92bed0c1f3b",
   "metadata": {},
   "source": [
    "### **Ans.** Correlation is a statistical measure that describes the relationship between two variables. \n",
    "### It indicates how one variable changes in relation to another and is expressed as a value between -1 and +1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6bba32-8fce-4630-8418-d2845edd9f8b",
   "metadata": {},
   "source": [
    "### Negative correlation means that as one variable increases, the other variable decreases, and vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc10f0e9-6132-424f-8e0f-d73f782bf701",
   "metadata": {},
   "source": [
    "### Q3. Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f0ac1-6eca-42f9-ae2d-772956899dd6",
   "metadata": {},
   "source": [
    "### **Answer:** Machine Learning (ML) is a branch of artificial intelligence (AI) that focuses on building systems capable of learning and improving from experience without being explicitly programmed. It uses algorithms and statistical models to identify patterns in data and make predictions or decisions.\n",
    "\n",
    "### The main components of machine learning are Data, Features, Model, Algorithm, Objective/Loss Function, Optimization, Evaluation, and Prediction/Inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99ed96-1fd7-4872-9501-dcdccc81dac2",
   "metadata": {},
   "source": [
    "### Q4. How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa537048",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### **answer:** The loss value measures how well the model's predictions match the actual target values. A lower loss indicates better model performance, while a higher loss suggests poor predictions. By monitoring the loss during training and evaluation, we can assess if the model is learning effectively, detect issues like underfitting or overfitting, and compare different models or configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ca489-21d2-4e69-973d-e71ad05746ac",
   "metadata": {},
   "source": [
    "### Q5) What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b476d19-fd22-4de7-ad18-70a06202f879",
   "metadata": {},
   "source": [
    "### **Answer:**\n",
    "- **Continuous Variables:** These are variables that can take any value within a range, often representing measurable quantities. Examples include height, weight, and temperature. They are typically represented by real numbers and allow for mathematical operations like addition and division.\n",
    "- **Categorical Variables:** These variables represent distinct categories or groups. They can be nominal (unordered, e.g., gender, colors) or ordinal (ordered, e.g., education levels). Categorical variables are usually represented by labels or codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650f72a-74cc-436a-bc14-1839f31a7789",
   "metadata": {},
   "source": [
    "### Q6) How do we handle categorical variables in Machine Learning? What are the common techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e9601",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### **Answer:** To handle categorical variables in machine learning, common techniques include:\n",
    "\n",
    "1. Label Encoding: Assigns unique integers to each category (suitable for ordinal data).\n",
    "2. One-Hot Encoding: Creates binary columns for each category (suitable for nominal data).\n",
    "3. Binary Encoding: Converts categories to binary numbers and splits them into columns, reducing dimensionality.\n",
    "4. Frequency Encoding: Replaces categories with their frequency in the dataset.\n",
    "5. Target Encoding: Replaces categories with the mean of the target variable for each category.\n",
    "6. Embedding Layers: Used in neural networks to map categories to dense vectors.\n",
    "7. Hash Encoding: Maps categories to a fixed number of columns using a hash function, useful for high-cardinality features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59591d64-01a3-43e0-a2ac-08cdc1564de5",
   "metadata": {},
   "source": [
    "### Q7) What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722c622-d737-49c0-ba7c-3d22f4569d6e",
   "metadata": {},
   "source": [
    "### **Answer:**\n",
    " - **1. Training a Dataset:** This involves feeding a portion of the data (training set) to the model, allowing it to learn patterns and adjust itsparameters to minimize errors. It helps the model make predictions or classifications.\n",
    "\n",
    " - **2. Testing a Dataset:** This involves evaluating the trained model on a separate portion of data (test set) to assess its ability to generalize to new,      unseen data. It measures the model's performance using metrics like accuracy or precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d084c68-ae6f-4968-a47e-3f42efe30aa3",
   "metadata": {},
   "source": [
    "### Q8) What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbdb95b-f292-41cf-af6e-97f8641d078a",
   "metadata": {},
   "source": [
    "### **Answer:** sklearn.preprocessing is a module in scikit-learn that provides tools for preprocessing data, including scaling, encoding categorical variables, handling missing values, and creating polynomial features, to make the data suitable for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae5664-1dfd-4352-a4b2-b56e4095dbee",
   "metadata": {},
   "source": [
    "### Q9) What is a Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a241d7-5c4b-481a-860f-a537da51c3e4",
   "metadata": {},
   "source": [
    "### **Answer:** A test set is a portion of the dataset that is used to evaluate the performance of a machine learning model after it has been trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5402a48-efb0-4b93-9228-ac2a990ae528",
   "metadata": {},
   "source": [
    "### Q10) How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa929b95",
   "metadata": {},
   "source": [
    "### **Answer:**\n",
    "\n",
    "### To split data for model fitting (training and testing) in Python, use train_test_split from sklearn.model_selection.\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "### Example:\n",
    "X = features, y = target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "### This splits the data into 80% training and 20% testing sets.\n",
    "\n",
    "### **Approach to a Machine Learning problem:**\n",
    "- 1. Understand the problem and define the objective.\n",
    "- 2. Collect and explore the data.\n",
    "- 3. Preprocess and clean the data.\n",
    "- 4. Perform feature engineering.\n",
    "- 5. Select and train a model.\n",
    "- 6. Evaluate the model.\n",
    "- 7. Tune and improve the model.\n",
    "- 8. Deploy and monitor the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e94236-a724-47fd-977d-2e25883074aa",
   "metadata": {},
   "source": [
    "### Q11) Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9862c21-095f-46bf-bfbe-2a8761408977",
   "metadata": {},
   "source": [
    "### **Answer:** Exploratory Data Analysis (EDA) is performed before fitting a model to the data to gain insights and ensure the data is ready for modeling. Here are the key reasons why EDA is important:\n",
    "\n",
    "### **EDA (Exploratory Data Analysis) is essential before modeling to:**\n",
    "\n",
    "- 1. Understand data structure and relationships.\n",
    "\n",
    "- 2. Detect missing, invalid data, and outliers.\n",
    "- 3. Inform feature selection and transformations.\n",
    "- 4. Check model assumptions.\n",
    "- 5. Select the appropriate model for the data.\n",
    "- 6. It ensures the data is clean, well-prepared, and suitable for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d256274-941f-493d-afe3-d682ad00ab83",
   "metadata": {},
   "source": [
    "### Q12) What is correlation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873e02b-5746-4275-a880-8cff00893ff9",
   "metadata": {},
   "source": [
    "### **Answer:** Correlation is a statistical measure that describes the relationship between two variables. It indicates how one variable changes in relation to another and is expressed as a value between -1 and +1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e71fd1-839a-4204-b347-33bff2faac11",
   "metadata": {},
   "source": [
    "### Q13) What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba8913-560d-41c4-95a9-d7700d2b1f43",
   "metadata": {},
   "source": [
    "### **Answer** Negative correlation means that as one variable increases, the other variable decreases, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981dac0-db21-452c-94f9-ade02b105d38",
   "metadata": {},
   "source": [
    "### Q14) How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4197c",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### **answer:** we can find correlation between variables in Python using the .corr() method in pandas.\n",
    "### For example, given a DataFrame 'df', we can compute the correlation matrix as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8c299c-1ad0-4bc7-a590-ebe42b1996bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Variable1  Variable2  Variable3\n",
      "Variable1   1.000000   1.000000  -0.686244\n",
      "Variable2   1.000000   1.000000  -0.686244\n",
      "Variable3  -0.686244  -0.686244   1.000000\n"
     ]
    }
   ],
   "source": [
    "# Example dataset\n",
    "import pandas as pd\n",
    "data = {\n",
    "    'Variable1': [1, 2, 3, 4, 5],\n",
    "    'Variable2': [2, 4, 6, 8, 10],\n",
    "    'Variable3': [5, 3, 6, 2, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b19c7-0576-4ddd-81e8-cdb2345b4ed0",
   "metadata": {},
   "source": [
    "### **Using NumPy**\n",
    "### The numpy.corrcoef() function calculates the Pearson correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f47803-9b82-4ebd-9592-4f389d2cd052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example variables\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Correlation coefficient\n",
    "correlation = np.corrcoef(x, y)\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d13450-c3ba-4dac-8780-cc61c0f30d98",
   "metadata": {},
   "source": [
    "### Q15)  What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b75e6-7ddd-4a61-9e12-d8115f185984",
   "metadata": {},
   "source": [
    "### **Answer** Causation means that one event directly causes another. In other words, a change in one variable (the cause) directly results in a change in another variable (the effect). Establishing causation requires evidence of a direct cause-and-effect relationship.\n",
    "\n",
    "### **Difference Between Correlation and Causation**\n",
    "\n",
    "### 1. Correlation: Measures the statistical relationship or association between two variables. Correlation does not imply that one variable causes the other to change. It simply shows that the variables move together in some way (positive, negative, or no relationship).\n",
    "\n",
    "### 2. Causation: Indicates that one variable's change directly leads to a change in another variable. It implies a cause-and-effect relationship.\n",
    "\n",
    "- **Example**, Suppose we find a positive correlation between ice cream sales and drowning incidents:\n",
    "\n",
    "### When ice cream sales increase, drowning incidents also increase.\n",
    "### This correlation does not imply causation. It is likely that a third variable, such as hot weather, causes both:\n",
    "\n",
    "### Hot weather increases ice cream sales.\n",
    "### Hot weather also leads to more people swimming, which can increase the likelihood of drowning.\n",
    "### Here, the relationship between ice cream sales and drowning incidents is correlation, not causation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d2731-b2c8-412f-bf7a-941538d7f0db",
   "metadata": {},
   "source": [
    "### Q16)  What is an Optimizer? What are different types of optimizers? Explain each with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067e8136",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### **Answer:**  \n",
    "An optimizer is an algorithm used in machine learning and deep learning to adjust the parameters (weights and biases) of a model to minimize the loss function and improve model performance. Optimizers update the model parameters based on the computed gradients during training.\n",
    "\n",
    "#### **Types of Optimizers:**\n",
    "\n",
    "1. **Gradient Descent:**  \n",
    "    Updates parameters in the direction of the negative gradient of the loss function.  \n",
    "    - **Variants:**  \n",
    "      - **Batch Gradient Descent:** Uses the entire dataset for each update.  \n",
    "      - **Stochastic Gradient Descent (SGD):** Updates parameters for each training example.  \n",
    "      - **Mini-Batch Gradient Descent:** Uses small batches of data for each update.\n",
    "\n",
    "    **Example (SGD):**\n",
    "    ```python\n",
    "    # Pseudo code for SGD\n",
    "    weight = weight - learning_rate * gradient\n",
    "    ```\n",
    "\n",
    "2. **Momentum:**  \n",
    "    Accelerates SGD by adding a fraction of the previous update to the current update, helping to navigate ravines and avoid oscillations.\n",
    "\n",
    "    **Example:**\n",
    "    ```python\n",
    "    # Pseudo code for Momentum\n",
    "    v = beta * v + learning_rate * gradient\n",
    "    weight = weight - v\n",
    "    ```\n",
    "\n",
    "3. **Adagrad:**  \n",
    "    Adapts the learning rate for each parameter based on the sum of the squares of past gradients, making larger updates for infrequent parameters.\n",
    "\n",
    "    **Example:**\n",
    "    ```python\n",
    "    # Pseudo code for Adagrad\n",
    "    G = G + gradient**2\n",
    "    weight = weight - (learning_rate / (sqrt(G) + epsilon)) * gradient\n",
    "    ```\n",
    "\n",
    "4. **RMSprop:**  \n",
    "    Similar to Adagrad but uses a moving average of squared gradients to normalize the learning rate, preventing it from shrinking too much.\n",
    "\n",
    "    **Example:**\n",
    "    ```python\n",
    "    # Pseudo code for RMSprop\n",
    "    E[g^2] = decay_rate * E[g^2] + (1 - decay_rate) * gradient**2\n",
    "    weight = weight - (learning_rate / (sqrt(E[g^2]) + epsilon)) * gradient\n",
    "    ```\n",
    "\n",
    "5. **Adam (Adaptive Moment Estimation):**  \n",
    "    Combines Momentum and RMSprop by maintaining both the average of past gradients and the average of past squared gradients.\n",
    "\n",
    "    **Example:**\n",
    "    ```python\n",
    "    # Pseudo code for Adam\n",
    "    m = beta1 * m + (1 - beta1) * gradient\n",
    "    v = beta2 * v + (1 - beta2) * gradient**2\n",
    "    m_hat = m / (1 - beta1**t)\n",
    "    v_hat = v / (1 - beta2**t)\n",
    "    weight = weight - (learning_rate * m_hat) / (sqrt(v_hat) + epsilon)\n",
    "    ```\n",
    "\n",
    "- Each optimizer has its own strengths and is chosen based on the problem and dataset characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efb1c1-4270-4751-8668-ba9b575c256c",
   "metadata": {},
   "source": [
    "### Q17)  What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95fe14d-b5a7-4291-8ae0-3505e5f32b09",
   "metadata": {},
   "source": [
    "### **Answer** sklearn.linear_model is a module in scikit-learn, a popular Python library for machine learning, that provides tools for implementing linear models. Linear models are used for predicting a target variable by fitting a linear relationship between the input features and the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351eab4-3372-4b6d-ac2e-299958616a60",
   "metadata": {},
   "source": [
    "Q18) What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae34265-eeba-41cc-8e44-a56459e0135e",
   "metadata": {},
   "source": [
    "Ans. The model.fit() method in scikit-learn is used to train a machine learning model on the given data. It:\n",
    "\n",
    "1. Fits the model to the training data by estimating the parameters (e.g., weights, biases).\n",
    "   \n",
    "2. Learns the relationship between input features (X) and the target variable (y).\n",
    "\n",
    "\n",
    "3. Stores the learned parameters in the model instance for later use (e.g., prediction).\n",
    "\n",
    "Arguments for model.fit():\n",
    "X: Type: 2D array or matrix.\n",
    "Shape: (n_samples, n_features).\n",
    "Purpose: Input features.\n",
    "\n",
    "y: Type: 1D or 2D array.\n",
    "Shape: (n_samples,) or (n_samples, n_targets).\n",
    "Purpose: Target variable(s).\n",
    "\n",
    "Optional:\n",
    "sample_weight: Weights for samples.\n",
    "Model-specific parameters like epochs or callbacks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43649c5-ecf8-4838-b36c-e062ed094b76",
   "metadata": {},
   "source": [
    "Q19) What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705b50d-b0a4-4010-92cf-dc7d5d2b22fd",
   "metadata": {},
   "source": [
    "Ans. The model.predict() method in scikit-learn is used to make predictions on new, unseen data after a model has been trained using model.fit(). It takes input features and outputs predicted values or labels based on the learned relationship.\n",
    "\n",
    "Arguments for model.predict():\n",
    "\n",
    "X: Type: 2D array or matrix.\n",
    "Shape: (n_samples, n_features).\n",
    "Purpose: Input features for which predictions are made.\n",
    "\n",
    "Output:\n",
    "Regression: Continuous values.\n",
    "Classification: Class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91dc73a-ac9d-44c3-a904-b2d9ebaf176c",
   "metadata": {},
   "source": [
    "Q20) What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60628d-061e-4fb1-860e-40d7ec1782a2",
   "metadata": {},
   "source": [
    "Ans. 1. Continuous Variables :\n",
    "\n",
    "Definition: Continuous variables are quantitative variables that can take any value within a given range or interval.\n",
    "\n",
    "Examples:\n",
    "Temperature (e.g., 22.5°C, 25.8°C)\n",
    "Height (e.g., 5.8 feet, 6.1 feet)\n",
    "Weight (e.g., 70.5 kg, 80.3 kg)\n",
    "\n",
    "2. Categorical Variables :\n",
    "   \n",
    "Definition: Categorical variables represent categories or groups that are often qualitative in nature.\n",
    "\n",
    "Examples:\n",
    "Nominal:\n",
    "Colors (e.g., Red, Blue, Green)\n",
    "Gender (e.g., Male, Female, Other)\n",
    "\n",
    "Ordinal:\n",
    "Education level (e.g., High School, Bachelor's, Master's)\n",
    "Rating scale (e.g., Poor, Average, Good, Excellent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31855a2-9c65-4a4c-9602-86a58d9eaf2d",
   "metadata": {},
   "source": [
    "Q21) What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617bac6-0646-4109-941f-0557a584d791",
   "metadata": {},
   "source": [
    "Ans. Feature scaling is the process of transforming the features (input variables) in a dataset to a common scale or range\n",
    "It involves rescaling the features so that they are on a similar scale, making the model training process more efficient and improving performance.\n",
    "\n",
    "Why Feature Scaling is Important in Machine Learning\n",
    "\n",
    "1. Improves Model Performance:\n",
    "\n",
    "Gradient-based algorithms (e.g., Gradient Descent, Logistic Regression, Neural Networks) converge faster when features are scaled because they use the gradient of the loss function to adjust model parameters.\n",
    "\n",
    "Distance-based algorithms (e.g., K-Nearest Neighbors (KNN), Support Vector Machines (SVM)) rely on distances between data points. If one feature has much larger values than another, it can dominate the distance calculations and negatively affect the model's performance.\n",
    "\n",
    "2. Prevents Dominance of Features:\n",
    "\n",
    "Features with larger numerical ranges can overshadow those with smaller ranges, affecting how the model learns.\n",
    "\n",
    "3. Improves Interpretability:\n",
    "\n",
    "Scaling features to the same range can help interpret the importance of each feature in models like linear regression, where coefficients represent feature importance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49ca41-a36f-4a30-8430-6eb36947604d",
   "metadata": {},
   "source": [
    "### Q22) How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1103bb-223c-4569-9c7f-1a1c6b32b2e8",
   "metadata": {},
   "source": [
    "### **Answer** In Python, scaling of features is typically done using the scikit-learn library, which provides various preprocessing functions for scaling. Here’s how you can perform scaling in Python using different methods:\n",
    "\n",
    "- ### **1. Min-Max Scaling (Normalization)**\n",
    "### This method rescales the features to a fixed range, typically [0, 1].\n",
    "\n",
    "#### **Steps:**\n",
    "- Use MinMaxScaler from sklearn.preprocessing.\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data (features)\n",
    "X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Output the scaled features\n",
    "print(X_scaled)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c17dd-d998-479a-89a4-bf0630a3f864",
   "metadata": {},
   "source": [
    "### Q23) What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d747743d-ab15-48af-8ef0-bb1e24a2b972",
   "metadata": {},
   "source": [
    "### **Answer:** \n",
    "- 1. sklearn.preprocessing is a module in scikit-learn that provides a suite of functions and classes to preprocess data before it is used to train machine learning models. These preprocessing techniques are essential for transforming data into a format that can help machine learning algorithms perform optimally.\n",
    "\n",
    "- 2. sklearn.preprocessing provides tools to scale, normalize, and transform data.\n",
    "\n",
    "- 3. It helps convert raw data into a form suitable for machine learning algorithms to achieve better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d3c8ed-eafd-49af-8228-5911d30b74c4",
   "metadata": {},
   "source": [
    "Q24) How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a952c-8d53-41db-b00d-5f53727941cb",
   "metadata": {},
   "source": [
    "### **Answe:** To split data into training and testing sets in Python, you typically use the train_test_split() function from scikit-learn. \n",
    "### This function randomly splits the dataset into two subsets: one for training the model and the other for testing its performance.\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]  # Features\n",
    "y = [0, 1, 0, 1, 0]  # Target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20691ced-0eec-4d5c-8081-2323f50b8e00",
   "metadata": {},
   "source": [
    "### Q25) Explain data encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d847cd5",
   "metadata": {},
   "source": [
    "### **Answer**\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "```\n",
    "\n",
    "### Data encoding is the process of converting categorical (non-numeric) data into a numerical format so that machine learning algorithms can use it.\n",
    "### Many algorithms require numeric input, so encoding is an essential preprocessing step.\n",
    "\n",
    "### **Common types of data encoding:**\n",
    "- 1. Label Encoding: Assigns each unique category a different integer value. Useful for ordinal data.\n",
    "- 2. One-Hot Encoding: Creates binary columns for each category. Useful for nominal data.\n",
    "- 3. Binary, Frequency, and Target Encoding: Other advanced techniques for specific scenarios.\n",
    "\n",
    "### **Example: Label Encoding**\n",
    "```python\n",
    "categories = ['Low', 'Medium', 'High', 'Medium', 'Low']\n",
    "encoder = LabelEncoder()\n",
    "encoded_data = encoder.fit_transform(categories)\n",
    "print(encoded_data)  # Output: [1 2 0 2 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4b820-9424-4854-a836-a0b45bd40232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
